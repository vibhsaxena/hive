Hive draws a distinction bw how records are encoded into files and how columns are encoded into records
	Record encoding is handled by an inputformat object (Hive uses a class called org.apache.hadoop.mapred.TextInputFormat)
	Record encoding is handled by a serializer/deserializer or SerDe
	There is also an output format that Hive uses for writing the output of queries to files and to the console

The SerDe interface allows us to instruct Hive as to how a record should be processed. The Deserializer interface takes a string or a binary representation of a record and translates it into a Java object that Hive can manipulate. The Serializer, takes a Java Object that Hive has been working on and turns it into something that Hive can write to HDFS/S3. 
Commonly, 
	Deserializers are used at query time to execute SELECT statements
	Serializers are used when writing data, such as through INSERT-SELECT statement.

For example, let's take the case of the analysing Twitter data example. When we get the data in from Flume, it is in JSON format. We get the JSON SerDe, add the JAR in our Hive path or when we are creating the tweets table


example of specifying custom specs:
CREATE TABLE test
PARTITIONED BY (tester string)
RAW FORMAT SERDE 'com.linkedin.haivvreo.AvroSerDe'
WITH SERDEPROPERTIES ('schema.url='http://schemaprovider/test.avsc'')
STORED AS 
INPUTFORMAT 'com.linked.haivvreo.AvroContainerInputFormat'
OUTPUTFORMAT 'com.linked.haivvreo.AvroContainerOutputFormat'

*Alter table statement only tinkers the metadata of the table, not the data
